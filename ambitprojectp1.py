# -*- coding: utf-8 -*-
"""AmbitProjectP1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1h-JuYuWzENed8Sne_GzNSncmXzx60n3f
"""

import pandas as pd

#Reading file
df = pd.read_csv('/content/drive/MyDrive/Tower_Sentiment.csv')
#Dropping columns that do not need to be read
df.drop(df.columns[[4,5,8,9]], axis=1, inplace=True)
df.head()

#Current amount of rows in dataset
print(len(df))
#dropping rows after 1500 because thats where we annotated to
#df.drop(df.index[1500:], inplace=True)
print(len(df))
#Dropping all NaN data points
df_filled = df.dropna()
#Removing all mistakes from the dataset where names were not spelt right
specific_word = 'Neural'
df_filled = df_filled[~df_filled['Sentiment'].str.contains(specific_word, case=False)]
specific_word = 'Positve'
df_filled = df_filled[~df_filled['Sentiment'].str.contains(specific_word, case=False)]
print(len(df_filled))
df_filled.head()

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
#train and test data split
train_data, test_data = train_test_split(df_filled, test_size=0.2, random_state=42)

vectorizer = TfidfVectorizer(max_features=5000)  # You can adjust max_features as needed

# Fit and transform the vectorizer on the training data
X_train = vectorizer.fit_transform(train_data['Text'])
y_train = train_data['Sentiment']

# Transform the vectorizer on the test data
X_test = vectorizer.transform(test_data['Text'])
y_test = test_data['Sentiment']

from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC

#LinearRegression
linearR = LinearRegression()
#Create a Naive Bayes classifier
classifier = MultinomialNB()
#LogisticRegression
logistic = LogisticRegression()
#Decision Tree
decision = DecisionTreeClassifier()
#Random forest
forest = RandomForestClassifier()
#SVM
svm = SVC()

# Train the classifier on the training data
#linearR.fit(X_train, y_train)
classifier.fit(X_train, y_train)
logistic.fit(X_train, y_train)
decision.fit(X_train, y_train)
forest.fit(X_train, y_train)
svm.fit(X_train, y_train)

from sklearn.metrics import accuracy_score, classification_report

# Predict on the test data
y_pred = classifier.predict(X_test)
# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Naive Bayes:", accuracy)

# l_pred = linearR.predict(X_test)
# # Calculate accuracy
# accuracy = accuracy_score(y_test, l_pred)
# print("Linear Regression:", accuracy)

logistic_pred = logistic.predict(X_test)
# Calculate accuracy
accuracy = accuracy_score(y_test, logistic_pred)
print("Logistic Regression:", accuracy)

d_pred = decision.predict(X_test)
# Calculate accuracy
accuracy = accuracy_score(y_test, d_pred)
print("Decision Tree:", accuracy)

f_pred = forest.predict(X_test)
# Calculate accuracy
accuracy = accuracy_score(y_test, f_pred)
print("Random Forest:", accuracy)

svm_pred = svm.predict(X_test)
# Calculate accuracy
accuracy = accuracy_score(y_test, svm_pred)
print("Svm:", accuracy)

# Generate a classification report
print("Classification Report:")
print(classification_report(y_test, y_pred))

new_text = "hi there"
new_text_features = vectorizer.transform([new_text])
predicted_sentiment = classifier.predict(new_text_features)[0]

print("Predicted Sentiment:", predicted_sentiment)


# -*- coding: utf-8 -*-
"""AmbitProjectP2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jdKVypdfMYJZqXjDr8uuvMgSKN0ZPstR
"""

pip install profanity

import pandas as pd
import numpy as np

from profanity import profanity
profanity.censor("this")

#bad words
bad_words = profanity.words
bad_words.append("fuck")
bad_words.append("bitch")
bad_words.append("fuck")
bad_words.append("suck")
bad_words.append("hate")
bad_words.append("hate")
bad_words.append("hate")



bad_words.extend(bad_words)
print(bad_words)
len(bad_words)

#good words
good_words = ["good", "love", "lovely", "thanks", "Thank you", "thank you so much", "appreciate", "great"]
good_words.extend(good_words)
print(good_words)

#Single text file
#Reading file
# Read the CSV file
df = pd.read_csv('/content/drive/MyDrive/Tower_Sentiment.csv')

# Dropping columns that do not need to be read
df.drop(df.columns[[4, 5, 8, 9]], axis=1, inplace=True)

# Truncate the DataFrame to contain only the first 3007 rows
df = df.iloc[:3007]

# Create a DataFrame for the new words and sentiments
num_new_words = len(bad_words)
random_indices = np.random.randint(0, df.shape[0], num_new_words)

# Insert new data at the randomly selected indices
for i, word in enumerate(bad_words):
    df = pd.concat([df.iloc[:random_indices[i]], pd.DataFrame({'Text': [word], 'Sentiment': ['Negative']}), df.iloc[random_indices[i]:]], ignore_index=True)

for i, word in enumerate(good_words):
    df = pd.concat([df.iloc[:random_indices[i]], pd.DataFrame({'Text': [word], 'Sentiment': ['Positive']}), df.iloc[random_indices[i]:]], ignore_index=True)
print("\nUpdated Dataset:")
print(df.tail())

result = df['Text'].str.contains("hate")
filtered_df = df[result]

print(filtered_df)

result = df['Text'].str.contains("love")
filtered_df = df[result]

print(filtered_df)

"""user: sadaa
bot: adsda

"""

# import pandas as pd

# # Read the CSV file and skip the first row
# df2 = pd.read_csv('/content/drive/MyDrive/Tower_Sentiment.csv', skiprows=0)

# # Drop columns that do not need to be read
# df2.drop(df2.columns[[4, 5]], axis=1, inplace=True)

# df2.head()

# from google.colab import drive
# drive.mount('/content/drive')

# df2['Timestamp'] = pd.to_datetime(df2['Timestamp'])

# # Group by 'Conversation ID' and aggregate 'Text' by joining based on 'Source' and preserving timestamp order
# def concatenate_text(group):
#     combined_text = ', '.join([f"{source.capitalize()}: {text}" for _, source, text in group[['Timestamp', 'Source', 'Text']].itertuples(index=False)])
#     return pd.Series({
#         'Conversation ID': group['Conversation ID'].iloc[0],
#         'Combined_Text': combined_text
#     })

# grouped_df = df2.groupby('Conversation ID').apply(concatenate_text).reset_index(drop=True)

# print(grouped_df)

# text_paragraph = grouped_df['Combined_Text'][6]
# print(text_paragraph)

#Current amount of rows in dataset
print(len(df))
#dropping rows after 1500 because thats where we annotated to
#df.drop(df.index[1500:], inplace=True)
print(len(df))
#Dropping all NaN data points
#df_filled = df.dropna()
df_filled = df.fillna(" ")
old_word = 'Neural'
new_word = 'Neutral'
# Find rows containing the target word and replace it with the new word
df_filled['Sentiment'] = df_filled['Sentiment'].replace(old_word, new_word)

old_word = 'Positve'
new_word = 'Positive'
# Find rows containing the target word and replace it with the new word
df_filled['Sentiment'] = df_filled['Sentiment'].replace(old_word, new_word)

old_word = 'positive'
new_word = 'Positive'
# Find rows containing the target word and replace it with the new word
df_filled['Sentiment'] = df_filled['Sentiment'].replace(old_word, new_word)

# Save the modified DataFrame back to the CSV file
df_filled.to_csv('/content/drive/MyDrive/Tower_Sentiment_New.csv', index=False)

#Removing all mistakes from the dataset where names were not spelt right
specific_word = 'Neural'
df_filled = df_filled[~df_filled['Sentiment'].str.contains(specific_word, case=False)]
specific_word = 'Positve'
df_filled = df_filled[~df_filled['Sentiment'].str.contains(specific_word, case=False)]
print(len(df_filled))
df_filled.head()

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
#train and test data split
train_data, test_data = train_test_split(df_filled, test_size=0.3, random_state=42)

vectorizer = TfidfVectorizer(max_features=5000)  # You can adjust max_features as needed

# Fit and transform the vectorizer on the training data
X_train = vectorizer.fit_transform(train_data['Text'])
y_train = train_data['Sentiment']

# Transform the vectorizer on the test data
X_test = vectorizer.transform(test_data['Text'])
y_test = test_data['Sentiment']

from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC

#LinearRegression
linearR = LinearRegression()
#Create a Naive Bayes classifier
classifier = MultinomialNB()
#LogisticRegression
logistic = LogisticRegression()
#Decision Tree
decision = DecisionTreeClassifier()
#Random forest
forest = RandomForestClassifier()
#SVM
svm = SVC()

# Train the classifier on the training data
#linearR.fit(X_train, y_train)
classifier.fit(X_train, y_train)
logistic.fit(X_train, y_train)
decision.fit(X_train, y_train)
forest.fit(X_train, y_train)
svm.fit(X_train, y_train)

from sklearn.metrics import accuracy_score, classification_report

# Predict on the test data
y_pred = classifier.predict(X_test)
# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Naive Bayes:", accuracy)

# l_pred = linearR.predict(X_test)
# # Calculate accuracy
# accuracy = accuracy_score(y_test, l_pred)
# print("Linear Regression:", accuracy)

logistic_pred = logistic.predict(X_test)
# Calculate accuracy
accuracy = accuracy_score(y_test, logistic_pred)
print("Logistic Regression:", accuracy)

d_pred = decision.predict(X_test)
# Calculate accuracy
accuracy = accuracy_score(y_test, d_pred)
print("Decision Tree:", accuracy)

f_pred = forest.predict(X_test)
# Calculate accuracy
accuracy = accuracy_score(y_test, f_pred)
print("Random Forest:", accuracy)

svm_pred = svm.predict(X_test)
# Calculate accuracy
accuracy = accuracy_score(y_test, svm_pred)
print("Svm:", accuracy)

# Generate a classification report
print("Naive Bayes Report:")
print(classification_report(y_test, y_pred))

print("Logistic Regression Report:")
print(classification_report(y_test, logistic_pred))

print("Decision Tree Report:")
print(classification_report(y_test, d_pred))

print("Random Forest Report:")
print(classification_report(y_test, f_pred))

print("SVM Report:")
print(classification_report(y_test, svm_pred))

new_text = "i think this is great"
new_text_features = vectorizer.transform([new_text])

#Bayes
predicted_sentiment = classifier.predict(new_text_features)[0]
print("Predicted Sentiment Bayes:", predicted_sentiment)
#logistic
predicted_sentiment = logistic.predict(new_text_features)[0]
print("Predicted Sentiment logistic:", predicted_sentiment)
#Decision treee
predicted_sentiment = decision.predict(new_text_features)[0]
print("Predicted Sentiment Decision tree:", predicted_sentiment)
#Forest
predicted_sentiment = forest.predict(new_text_features)[0]
print("Predicted Sentiment forest:", predicted_sentiment)
#Svm
predicted_sentiment = svm.predict(new_text_features)[0]
print("Predicted Sentiment Svm:", predicted_sentiment)

